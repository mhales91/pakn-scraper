# .github/workflows/scrape.yml
name: scrape

on:
  workflow_dispatch:
  schedule:
    - cron: "0 1 * * *"   # daily at 01:00 UTC

jobs:
  scrape:
    runs-on: ubuntu-24.04
    permissions:
      contents: read

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup .NET 6 SDK
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: 6.0.x

      - name: Restore packages
        working-directory: src
        run: dotnet restore

      - name: Build (Debug) so playwright.ps1 exists
        working-directory: src
        run: dotnet build -c Debug

      - name: Install Playwright Chromium (per README)
        working-directory: src
        shell: pwsh
        run: bin/Debug/net6.0/playwright.ps1 install chromium

      # --- Diagnostics to prove where we are and what files exist
      - name: List important paths
        run: |
          echo "PWD = $(pwd)"
          echo "=== src ==="
          ls -la src
          echo "=== bin/Debug/net6.0 ==="
          ls -la src/bin/Debug/net6.0 || true

      # --- Copy Urls.txt to the build output where the app likely reads from
      - name: Ensure Urls.txt is in runtime folder
        run: |
          test -f src/Urls.txt || (echo "src/Urls.txt missing!" && exit 1)
          cp -f src/Urls.txt src/bin/Debug/net6.0/Urls.txt
          echo "After copy:"
          ls -la src/bin/Debug/net6.0 | grep -i Urls.txt || true

      # Run the scraper; capture stdout/stderr to files at repo root
      - name: Run scraper (dry run)
        working-directory: src
        run: |
          set -o pipefail
          { dotnet run ; } \
            >  ../out.stdout.txt \
            2> ../out.stderr.txt

      - name: Upload outputs
        uses: actions/upload-artifact@v4
        with:
          name: scrape-outputs
          path: |
            out.stdout.txt
            out.stderr.txt
            src/Urls.txt
          if-no-files-found: warn
