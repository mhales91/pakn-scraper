name: Daily Pak'nSave Scrape → Google Sheets

on:
  schedule:
    - cron: "0 18 * * *" # daily 18:00 UTC
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Debug mark
        run: |
          echo "HIT THE RIGHT WORKFLOW ✅"
          echo "BRANCH: $GITHUB_REF / SHA: $GITHUB_SHA"
          echo "DISPLAY is: ${DISPLAY:-<unset>}"

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '6.0.x'

      - name: Install Playwright CLI
        run: dotnet tool install --global Microsoft.Playwright.CLI

      - name: Add Playwright CLI to PATH
        run: echo "$HOME/.dotnet/tools" >> $GITHUB_PATH

      - name: Restore & Build
        run: |
          set -euxo pipefail
          dotnet restore src/PakScraper.csproj
          dotnet build src/PakScraper.csproj -c Release

      - name: Install Chromium for Playwright
        working-directory: src/bin/Release/net6.0
        run: pwsh ./playwright.ps1 install chromium

      - name: Install Xvfb
        run: |
          set -euxo pipefail
          sudo apt-get update
          sudo apt-get install -y xvfb

      # We capture STDOUT to out.stdout.txt for diagnostics,
      # but we DO NOT overwrite out.json anymore (in case your app writes its own JSON file).
      - name: Run the scraper
        run: |
          set -euxo pipefail
          xvfb-run -a -s "-screen 0 1280x720x24" \
            dotnet run --project src/PakScraper.csproj -- headless | tee out.stdout.txt
          echo "Scraper finished."

      # Discover the most likely JSON file produced by the run (newest *.json anywhere under repo)
      - name: Discover JSON output
        id: discover
        run: |
          set -euxo pipefail
          echo "Listing files produced:"
          find . -maxdepth 3 -type f -printf '%TY-%Tm-%Td %TH:%TM:%TS %p\n' | sort -r | head -n 80

          # Find newest JSON file (repo root or within 3 levels)
          JSON_FILE="$(find . -maxdepth 3 -type f -name '*.json' -printf '%T@ %p\n' 2>/dev/null | sort -nr | head -n1 | awk '{print substr($0, index($0,$2))}')"
          if [ -n "${JSON_FILE}" ] && [ -f "${JSON_FILE}" ]; then
            echo "Found JSON candidate: ${JSON_FILE}"
            echo "SCRAPER_JSON=${JSON_FILE}" >> "$GITHUB_ENV"
          else
            echo "No .json file found. We'll try to parse stdout instead."
            echo "SCRAPER_JSON=" >> "$GITHUB_ENV"
          fi

          echo "out.stdout.txt size:"
          wc -c out.stdout.txt || true

          echo "First 120 lines of out.stdout.txt:"
          sed -n '1,120p' out.stdout.txt || true

      - name: Upload raw outputs (for debugging)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-outputs
          path: |
            out.stdout.txt
            ${{ env.SCRAPER_JSON }}

      - name: Install Python deps for Sheets
        run: |
          python3 -m pip install --upgrade pip
          python3 -m pip install google-api-python-client pandas

      - name: Push to Google Sheets
        env:
          GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}
          SHEET_ID: ${{ secrets.SHEET_ID }}
          SHEET_TAB: ${{ secrets.SHEET_TAB }}
        run: |
          python3 - << 'PY'
          import os, json, sys, datetime
          import pandas as pd
          from google.oauth2.service_account import Credentials
          from googleapiclient.discovery import build

          # Choose input: discovered JSON file if present; otherwise parse stdout capture
          json_path = os.environ.get("SCRAPER_JSON", "").strip()
          source_label = ""
          raw = ""

          def read_text(path):
            with open(path, "r", encoding="utf-8", errors="replace") as f:
              return f.read().strip()

          if json_path and os.path.isfile(json_path):
            raw = read_text(json_path)
            source_label = f"file:{json_path}"
          else:
            # Fall back to parsing STDOUT (NDJSON or JSON block)
            if not os.path.isfile("out.stdout.txt"):
              print("No JSON file and no out.stdout.txt; nothing to ingest.", file=sys.stderr)
              sys.exit(0)
            raw = read_text("out.stdout.txt")
            source_label = "stdout"

          if not raw:
            print(f"Empty content from {source_label}", file=sys.stderr)
            sys.exit(0)

          data = []
          # Try strict JSON first
          try:
            j = json.loads(raw)
            data = j if isinstance(j, list) else [j]
          except json.JSONDecodeError:
            # Fallback: NDJSON / JSON-per-line inside the text
            for line in raw.splitlines():
              line=line.strip()
              if not line: 
                continue
              try:
                data.append(json.loads(line))
              except Exception:
                # ignore non-JSON lines
                pass

          if not data:
            print(f"No JSON records parsed from {source_label}.", file=sys.stderr)
            sys.exit(0)

          df = pd.json_normalize(data)
          now = datetime.datetime.utcnow().isoformat(timespec="seconds") + "Z"
          if "ingested_at" not in df.columns:
            df["ingested_at"] = now
          cols = list(df.columns)
          df = df[cols]

          creds = Credentials.from_service_account_info(
            json.loads(os.environ["GCP_SA_KEY"]),
            scopes=["https://www.googleapis.com/auth/spreadsheets"]
          )
          svc = build("sheets","v4",credentials=creds)

          values = [cols] + df.fillna("").astype(str).values.tolist()
          read = svc.spreadsheets().values().get(
            spreadsheetId=os.environ["SHEET_ID"],
            range=f"{os.environ.get('SHEET_TAB','Sheet1')}!A1:A1"
          ).execute()
          has_any = 'values' in read
          if has_any:
            values = values[1:]

          if values:
            svc.spreadsheets().values().append(
              spreadsheetId=os.environ["SHEET_ID"],
              range=f"{os.environ.get('SHEET_TAB','Sheet1')}!A1",
              valueInputOption="RAW",
              insertDataOption="INSERT_ROWS",
              body={"values": values}
            ).execute()
          print(f"Appended {len(values)} row(s) from {source_label}")
          PY
