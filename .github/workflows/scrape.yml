# .github/workflows/scrape.yml
name: scrape

on:
  workflow_dispatch:
  schedule:
    - cron: "0 1 * * *" # daily at 01:00 UTC - tweak as you like

jobs:
  scrape:
    runs-on: ubuntu-24.04
    permissions:
      contents: read

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup .NET 6 SDK
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: 6.0.x

      # Everything runs from ./src so the app finds Urls.txt as per README
      - name: Restore packages
        working-directory: src
        run: dotnet restore

      - name: Build (Debug) â€” required so playwright.ps1 exists in bin/Debug/net6.0
        working-directory: src
        run: dotnet build -c Debug

      - name: Install Playwright Chromium (per README)
        working-directory: src
        shell: pwsh
        run: bin/Debug/net6.0/playwright.ps1 install chromium

      # Optional: sanity check that Urls.txt is present
      - name: Check Urls.txt
        working-directory: src
        run: |
          test -f Urls.txt && echo "Found Urls.txt" || (echo "Urls.txt missing!" && exit 1)

      # Run the scraper. It prints a table to stdout (not JSON).
      - name: Run scraper (dry run)
        working-directory: src
        run: |
          set -o pipefail
          { dotnet run ; } \
            >  ../out.stdout.txt \
            2> ../out.stderr.txt

      - name: Upload outputs
        uses: actions/upload-artifact@v4
        with:
          name: scrape-outputs
          path: |
            out.stdout.txt
            out.stderr.txt
            src/Urls.txt
          if-no-files-found: warn
