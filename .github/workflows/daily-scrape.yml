name: Daily Pak'nSave Scrape → Google Sheets

on:
  schedule:
    - cron: "0 18 * * *"   # daily 18:00 UTC
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '6.0.x'

      - name: Install Playwright CLI
        run: dotnet tool install --global Microsoft.Playwright.CLI

      - name: Add Playwright CLI to PATH
        run: echo "$HOME/.dotnet/tools" >> "$GITHUB_PATH"

      - name: Restore & Build
        run: |
          set -euxo pipefail
          dotnet restore src/PakScraper.csproj
          dotnet build   src/PakScraper.csproj -c Release

      - name: Install Chromium for Playwright
        working-directory: src/bin/Release/net6.0
        run: pwsh ./playwright.ps1 install chromium

      - name: Install Xvfb
        run: |
          sudo apt-get update
          sudo apt-get install -y xvfb

      # ---------------- RUN SCRAPER ----------------
      - name: Run scraper and capture outputs
        run: |
          set -euxo pipefail

          # Run under virtual display. Redirect:
          #  - JSON/stdout → out.json
          #  - raw stdout  → out.stdout.txt
          #  - stderr      → out.stderr.txt
          # If your app writes JSON to a file instead of stdout, copy it to out.json afterwards.
          xvfb-run -a -s "-screen 0 1280x720x24" \
            bash -lc 'dotnet run --project src/PakScraper.csproj -- headless \
              > out.json 1> out.stdout.txt 2> out.stderr.txt || true'

          echo "===== tail stdout ====="
          tail -n 120 out.stdout.txt || true
          echo "===== tail stderr ====="
          tail -n 120 out.stderr.txt || true

          echo "===== file sizes ====="
          for f in out.json out.stdout.txt out.stderr.txt; do
            if [ -f "$f" ]; then
              echo -n "$f: "; wc -c "$f"
            else
              echo "$f: <missing>"
            fi
          done

          echo "===== workspace listing ====="
          ls -la

      # Always upload so you can download even when JSON is empty
      - name: Upload scrape artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scrape-outputs
          path: |
            out.json
            out.stdout.txt
            out.stderr.txt
          if-no-files-found: warn
          retention-days: 7

      # ---------------- PUSH TO SHEETS ----------------
      - name: Install Python deps
        run: |
          python3 -m pip install --upgrade pip
          python3 -m pip install google-api-python-client pandas

      - name: Push to Google Sheets (only if JSON exists & non-empty)
        if: ${{ hashFiles('out.json') != '' }}
        env:
          GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}
          SHEET_ID:   ${{ secrets.SHEET_ID }}
          SHEET_TAB:  ${{ secrets.SHEET_TAB }}
        run: |
          python3 - << 'PY'
          import os, json, sys, datetime
          import pandas as pd
          from google.oauth2.service_account import Credentials
          from googleapiclient.discovery import build

          raw = open("out.json","r",encoding="utf-8").read().strip()
          data = []
          if raw:
              try:
                  j = json.loads(raw)
                  data = j if isinstance(j, list) else [j]
              except json.JSONDecodeError:
                  for line in raw.splitlines():
                      line=line.strip()
                      if not line: continue
                      try: data.append(json.loads(line))
                      except: pass

          if not data:
              print("No JSON records parsed from scraper output.")
              sys.exit(0)

          df = pd.json_normalize(data)
          now = datetime.datetime.utcnow().isoformat(timespec="seconds")+"Z"
          if "ingested_at" not in df.columns:
              df["ingested_at"] = now
          cols = list(df.columns)
          df = df[cols]

          creds = Credentials.from_service_account_info(
              json.loads(os.environ["GCP_SA_KEY"]),
              scopes=["https://www.googleapis.com/auth/spreadsheets"]
          )
          svc = build("sheets","v4",credentials=creds)
          tab = os.environ.get("SHEET_TAB","Sheet1")

          read = svc.spreadsheets().values().get(
              spreadsheetId=os.environ["SHEET_ID"],
              range=f"{tab}!A1:A1"
          ).execute()
          has_any = 'values' in read

          values = [cols] + df.fillna("").astype(str).values.tolist()
          if has_any:
              values = values[1:]

          if values:
              svc.spreadsheets().values().append(
                  spreadsheetId=os.environ["SHEET_ID"],
                  range=f"{tab}!A1",
                  valueInputOption="RAW",
                  insertDataOption="INSERT_ROWS",
                  body={"values": values}
              ).execute()
              print(f"Appended {len(values)} row(s)")
          PY
